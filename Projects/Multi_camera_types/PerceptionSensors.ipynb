{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6af702-66a1-413e-99bc-43ad1278f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Objective: This code initializes required libraries for the Carla simulator to add a vehicle, and attach LiDAR and depth sensors to it. The libraries facilitate the simulation and visualization of the vehicle and sensor data.\n",
    "\n",
    "Libraries Overview:\n",
    "    - carla: Provides the interface to interact with the Carla simulator, allowing vehicle and sensor manipulation.\n",
    "    - math: Supplies mathematical functions (e.g., for positioning, rotations).\n",
    "    - random: Generates random numbers, useful for randomizing vehicle or sensor placement.\n",
    "    - time: Provides time control (e.g., for pauses and timing).\n",
    "    - numpy (np): Enables numerical operations on arrays, important for sensor data processing.\n",
    "    - cv2: Handles image processing and display (for camera sensor data).\n",
    "    - open3d as o3d: Used to visualize 3D point cloud data, essential for rendering LiDAR data.\n",
    "    - matplotlib and cm: Used for color mapping and data visualization (e.g., for depth sensor).\n",
    "'''\n",
    "import carla \n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import open3d as o3d # to mention\n",
    "from matplotlib import cm\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d182f57d-8131-4457-952b-db710195c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Establish a connection to the Carla simulator and access the simulation world.\n",
    "\n",
    "1. carla.Client('localhost', 2000):\n",
    "    - Purpose: Creates a client to connect to the Carla simulator running on localhost at port 2000.\n",
    "    - Usage: Acts as the main interface to communicate with the Carla server.\n",
    "\n",
    "2. client.get_world():\n",
    "    - Purpose: Retrieves the current simulation environment or \"world\" in Carla.\n",
    "    - Usage: Provides access to all elements in the simulation, such as vehicles, sensors, and actors, allowing manipulation within the world.\n",
    "'''\n",
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f35d267-2dfd-4bf2-96ad-062fdde225fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Retrieve available spawn points within the Carla simulation world.\n",
    "\n",
    "1.world.get_map():\n",
    "    - Purpose: Fetches the current map of the simulation world.\n",
    "    - Usage: Provides information related to the simulation environment's layout (e.g., roads, intersections).\n",
    "\n",
    "2.get_spawn_points():\n",
    "    - Purpose: Returns a list of predefined spawn points (positions and orientations) on the map where vehicles can be placed.\n",
    "    - Usage: Used to randomly or specifically select a location to spawn vehicles.\n",
    "'''\n",
    "spawn_points = world.get_map().get_spawn_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51aa7021-2433-483e-9963-a3872ad0ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Access the blueprint library to retrieve vehicle and sensor definitions.\n",
    "\n",
    "1. world.get_blueprint_library():\n",
    "    - Purpose: Fetches the blueprint library, which contains definitions for various actors, including vehicles, sensors, and props.\n",
    "    - Usage: Enables selection and instantiation of specific vehicle or sensor types within the simulation, allowing customization and flexibility in vehicle configurations.\n",
    "'''\n",
    "bp_lib = world.get_blueprint_library()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f21ec36-b889-4edb-9375-7a89586d7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Spawn a specific vehicle in the Carla simulation.\n",
    "\n",
    "1. `bp_lib.find('vehicle.tesla.cybertruck')`:\n",
    "   - Purpose: Locates the blueprint for the Tesla Cybertruck within the blueprint library.\n",
    "   - Usage: Retrieves the vehicle's specifications to allow for instantiation in the simulation.\n",
    "\n",
    "2. `world.try_spawn_actor(vehicle_bp, spawn_points[0])`:\n",
    "   - Purpose: Attempts to spawn an actor (the Tesla Cybertruck) at the first available spawn point.\n",
    "   - Usage: Creates an instance of the vehicle in the simulation; if spawning is successful, it returns the vehicle object, allowing further interaction.\n",
    "'''\n",
    "\n",
    "vehicle_bp = bp_lib.find('vehicle.tesla.cybertruck')\n",
    "vehicle = world.try_spawn_actor(vehicle_bp, spawn_points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44477d18-2b23-43df-939a-be0140fe11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Define the initial camera position for the simulation.\n",
    "\n",
    "1. `CAMERA_POS_Z = 3`:\n",
    "   - Purpose: Sets the Z-axis position of the camera above the vehicle.\n",
    "   - Usage: Specifies how high the camera will be placed relative to the vehicle, enhancing the field of view.\n",
    "\n",
    "2. `CAMERA_POS_X = 0`:\n",
    "   - Purpose: Sets the X-axis position of the camera.\n",
    "   - Usage: Defines the lateral position of the camera relative to the vehicle, allowing for adjustments in viewpoint.\n",
    "\n",
    "3. `camera_init_trans = carla.Transform(carla.Location(z = CAMERA_POS_Z, x = CAMERA_POS_X))`:\n",
    "   - Purpose: Creates a transformation object representing the camera's initial position.\n",
    "   - Usage: Combines the defined X and Z positions into a transform that can be used when attaching the camera to the vehicle.\n",
    "'''\n",
    "CAMERA_POS_Z = 3\n",
    "CAMERA_POS_X = 0\n",
    "camera_init_trans = carla.Transform(carla.Location(z = CAMERA_POS_Z, x = CAMERA_POS_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d91493b-2077-4f5e-a55f-95d37d1b36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Spawn an RGB camera sensor and attach it to the vehicle.\n",
    "\n",
    "1. `camera_bp = bp_lib.find('sensor.camera.rgb')`:\n",
    "   - Purpose: Locates the blueprint for the RGB camera within the blueprint library.\n",
    "   - Usage: Retrieves the specifications needed to instantiate the camera in the simulation.\n",
    "\n",
    "2. `camera = world.spawn_actor(camera_bp, camera_init_trans, attach_to=vehicle)`:\n",
    "   - Purpose: Spawns the RGB camera actor at the specified initial transform and attaches it to the vehicle.\n",
    "   - Usage: Ensures that the camera moves with the vehicle, allowing for real-time image capture during the simulation.\n",
    "'''\n",
    "camera_bp = bp_lib.find('sensor.camera.rgb')\n",
    "camera = world.spawn_actor(camera_bp, camera_init_trans, attach_to=vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb8a7243-0b94-46e7-b056-fdd8ae76a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Process and store incoming camera images.\n",
    "\n",
    "1. `def camera_callback(image, data_dict):`\n",
    "   - Purpose: Defines a callback function that processes raw image data from the camera.\n",
    "   - Parameters:\n",
    "     - `image`: The raw image data received from the camera sensor.\n",
    "     - `data_dict`: A dictionary where the processed image will be stored.\n",
    "\n",
    "2. Inside the function:\n",
    "   - `data_dict['image'] = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))`:\n",
    "     - Purpose: Copies the raw image data and reshapes it into a 4-channel image (including an alpha channel).\n",
    "     - `np.copy(image.raw_data)`: Creates a copy of the raw image data to prevent modifications to the original data.\n",
    "     - `np.reshape(..., (image.height, image.width, 4))`: Reshapes the data into a format suitable for further processing, where `image.height` and `image.width` correspond to the image's dimensions.\n",
    "\n",
    "3. Usage: This function allows real-time updates of the camera image data for visualization or analysis.\n",
    "\n",
    "'''\n",
    "def camera_callback(image, data_dict):\n",
    "    data_dict['image'] = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c589cb-f838-42ae-aab5-64c36deef4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. `depth_camera_bp = bp_lib.find('sensor.camera.depth')`:\n",
    "   - Purpose: Locates the blueprint for the depth camera within the blueprint library.\n",
    "   - Usage: Retrieves the specifications needed to instantiate the depth camera in the simulation.\n",
    "\n",
    "2. `depth_camera = world.spawn_actor(depth_camera_bp, camera_init_trans, attach_to=vehicle)`:\n",
    "   - Purpose: Spawns the depth camera actor at the specified initial transform and attaches it to the vehicle.\n",
    "   - Usage: Ensures that the depth camera moves with the vehicle, capturing depth data during the simulation.\n",
    "'''\n",
    "depth_camera_bp = bp_lib.find('sensor.camera.depth')\n",
    "depth_camera = world.spawn_actor(depth_camera_bp, camera_init_trans, attach_to = vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c095a4-9912-4a04-8dc4-7917601830e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Define a callback function to process depth camera images.\n",
    "\n",
    "1. `def depth_callback(image, data_dict):`:\n",
    "   - Purpose: Defines a function that is triggered when a depth image is received from the camera.\n",
    "   - Usage: Processes the incoming depth data and stores it in a dictionary for further analysis.\n",
    "\n",
    "2. `image.convert(carla.ColorConverter.LogarithmicDepth)`:\n",
    "   - Purpose: Converts the raw depth image data to a logarithmic scale.\n",
    "   - Usage: Enhances the visibility of depth information, making it easier to interpret.\n",
    "\n",
    "3. `data_dict['depth_image'] = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))`:\n",
    "   - Purpose: Copies and reshapes the processed raw depth data into a NumPy array.\n",
    "   - Usage: Converts the raw depth data into a format suitable for manipulation, matching the camera's resolution and including an alpha channel.\n",
    "'''\n",
    "\n",
    "def depth_callback(image, data_dict):\n",
    "    image.convert(carla.ColorConverter.LogarithmicDepth)\n",
    "    data_dict['depth_image'] = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b645fca-b91c-415e-aa5e-d4267d80141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Set up color maps and add a 3D axis to the Open3D visualizer.\n",
    "\n",
    "1. `VIRIDIS = np.array(matplotlib.colormaps.get_cmap('plasma').colors)`:\n",
    "   - Purpose: Retrieves the colors from the 'plasma' colormap and stores them as a NumPy array.\n",
    "   - Usage: Provides a set of colors for visualizing depth data or other metrics in the simulation.\n",
    "\n",
    "2. `VID_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])`:\n",
    "   - Purpose: Generates a linearly spaced array ranging from 0.0 to 1.0, with the same number of elements as the colors in the 'plasma' colormap.\n",
    "   - Usage: Serves as a range for mapping values to the colormap.\n",
    "\n",
    "3. `COOL_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])`:\n",
    "   - Purpose: Creates a similar range for the 'winter' colormap.\n",
    "   - Usage: Provides another set of colors for visualizing different data.\n",
    "\n",
    "4. `COOL = np.array(matplotlib.colormaps.get_cmap('winter')(COOL_RANGE))`:\n",
    "   - Purpose: Retrieves colors from the 'winter' colormap and stores them as a NumPy array.\n",
    "   - Usage: Offers a contrasting color palette for visualization.\n",
    "\n",
    "5. `COOL = COOL[:,:3]`:\n",
    "   - Purpose: Trims the 'winter' colormap to exclude the alpha channel, retaining only RGB values.\n",
    "   - Usage: Ensures compatibility with visualization libraries that require RGB input.\n",
    "\n",
    "6. `def add_open3d_axis(vis):`:\n",
    "   - Purpose: Defines a function to add a small 3D axis to the Open3D visualizer.\n",
    "   - Usage: Enhances the visual context by providing reference axes.\n",
    "\n",
    "7. Inside the function:\n",
    "   - **Creating a LineSet**: Sets up a 3D line set for the axes.\n",
    "   - **Defining Points and Lines**: Specifies the points and lines for the X, Y, and Z axes.\n",
    "   - **Setting Colors**: Colors the axes red, green, and blue for X, Y, and Z respectively.\n",
    "   - **Adding Geometry**: Adds the axis geometry to the Open3D visualizer.\n",
    "\n",
    "This setup enhances visualization by adding reference axes and establishing color maps for depth data interpretation.\n",
    "'''\n",
    "\n",
    "VIRIDIS = np.array(matplotlib.colormaps.get_cmap('plasma').colors)\n",
    "VID_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "\n",
    "COOL_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "COOL = np.array(matplotlib.colormaps.get_cmap('winter')(COOL_RANGE))\n",
    "COOL = COOL[:,:3]\n",
    "\n",
    "def add_open3d_axis(vis):\n",
    "    ''' Add a small 3D axis on Open3D visualizer '''\n",
    "    axis = o3d.geometry.LineSet()\n",
    "    axis.points = o3d.utility.Vector3dVector(np.array([\n",
    "        [0.0, 0.0, 0.0],\n",
    "        [1.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0]]))\n",
    "    axis.lines = o3d.utility.Vector2iVector(np.array([\n",
    "        [0, 1],\n",
    "        [0, 2],\n",
    "        [0, 3]]))\n",
    "    axis.colors = o3d.utility.Vector3dVector(np.array([\n",
    "        [1.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0]]))\n",
    "    vis.add_geometry(axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e850b6d7-70b2-4d0d-9ece-29e7d9f8f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Configure and spawn a LiDAR sensor, attaching it to the vehicle.\n",
    "\n",
    "1. `lidar_bp = bp_lib.find('sensor.lidar.ray_cast')`:\n",
    "   - Purpose: Locates the blueprint for the ray-cast LiDAR sensor within the blueprint library.\n",
    "   - Usage: Retrieves the specifications necessary for instantiating the LiDAR sensor in the simulation.\n",
    "\n",
    "2. `lidar_bp.set_attribute('range', '100.0')`:\n",
    "   - Purpose: Sets the maximum range of the LiDAR sensor to 100 meters.\n",
    "   - Usage: Defines how far the LiDAR can detect points in the environment.\n",
    "\n",
    "3. `lidar_bp.set_attribute('noise_stddev', '0.1')`:\n",
    "   - Purpose: Configures the standard deviation of noise added to the LiDAR measurements.\n",
    "   - Usage: Simulates real-world inaccuracies, making the sensor data more realistic.\n",
    "\n",
    "4. `lidar_bp.set_attribute('upper_fov', '15.0')`:\n",
    "   - Purpose: Sets the upper field of view (FOV) of the LiDAR sensor to 15 degrees.\n",
    "   - Usage: Determines the vertical range of detection for the sensor.\n",
    "\n",
    "5. `lidar_bp.set_attribute('lower_fov', '-25.0')`:\n",
    "   - Purpose: Sets the lower field of view (FOV) of the LiDAR sensor to -25 degrees.\n",
    "   - Usage: Extends the vertical range of detection below the vehicle.\n",
    "\n",
    "6. `lidar_bp.set_attribute('channels', '64.0')`:\n",
    "   - Purpose: Configures the number of channels for the LiDAR sensor to 64.\n",
    "   - Usage: Affects the resolution and detail of the point cloud data collected.\n",
    "\n",
    "7. `lidar_bp.set_attribute('rotation_frequency', '20.0')`:\n",
    "   - Purpose: Sets the rotation frequency of the LiDAR sensor to 20 Hz.\n",
    "   - Usage: Determines how quickly the LiDAR scans its surroundings.\n",
    "\n",
    "8. `lidar_bp.set_attribute('points_per_second', '500000')`:\n",
    "   - Purpose: Configures the LiDAR to emit 500,000 points per second.\n",
    "   - Usage: Influences the density of the point cloud data collected.\n",
    "\n",
    "9. `lidar_init_trans = carla.Transform(carla.Location(z=CAMERA_POS_Z, x=CAMERA_POS_X))`:\n",
    "   - Purpose: Creates a transformation object for the LiDAR's initial position.\n",
    "   - Usage: Sets the LiDAR's location above the vehicle using previously defined coordinates.\n",
    "\n",
    "10. `lidar = world.spawn_actor(lidar_bp, lidar_init_trans, attach_to=vehicle)`:\n",
    "    - Purpose: Spawns the LiDAR actor at the specified transform and attaches it to the vehicle.\n",
    "    - Usage: Ensures that the LiDAR moves with the vehicle, collecting point cloud data in real-time during the simulation.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "lidar_bp = bp_lib.find('sensor.lidar.ray_cast')\n",
    "lidar_bp.set_attribute('range', '100.0')\n",
    "lidar_bp.set_attribute('noise_stddev', '0.1')\n",
    "lidar_bp.set_attribute('upper_fov', '15.0')\n",
    "lidar_bp.set_attribute('lower_fov', '-25.0')\n",
    "lidar_bp.set_attribute('channels', '64.0')\n",
    "lidar_bp.set_attribute('rotation_frequency', '20.0')\n",
    "lidar_bp.set_attribute('points_per_second', '500000')\n",
    "\n",
    "lidar_init_trans = carla.Transform(carla.Location(z=CAMERA_POS_Z, x=CAMERA_POS_X))\n",
    "lidar = world.spawn_actor(lidar_bp, lidar_init_trans, attach_to = vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42914cb7-8d01-4d7b-bff9-49de006e15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Process LiDAR point cloud data and prepare it for visualization in Open3D.\n",
    "\n",
    "1. `def lidar_callback(point_cloud, point_list):`:\n",
    "   - Purpose: Defines a function to handle incoming LiDAR point cloud data.\n",
    "   - Usage: Prepares the data for visualization by converting it into a suitable format for Open3D.\n",
    "\n",
    "2. `data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4')))`:\n",
    "   - Purpose: Copies the raw point cloud data from the LiDAR sensor into a NumPy array.\n",
    "   - Usage: Converts the binary data into a format that can be reshaped and manipulated.\n",
    "\n",
    "3. `data = np.reshape(data, (int(data.shape[0] / 4), 4))`:\n",
    "   - Purpose: Reshapes the data array into a shape suitable for processing, with each point represented by four values (X, Y, Z, intensity).\n",
    "   - Usage: Prepares the data for further analysis by isolating the point coordinates and intensity.\n",
    "\n",
    "4. `intensity = data[:, -1]`:\n",
    "   - Purpose: Extracts the intensity values from the last column of the reshaped data.\n",
    "   - Usage: Used to compute a corresponding color for each point based on its intensity.\n",
    "\n",
    "5. `intensity_col = 1.0 - np.log(intensity) / np.log(np.exp(-0.004 * 100))`:\n",
    "   - Purpose: Normalizes the intensity values to a color scale.\n",
    "   - Usage: Converts intensity values into a range suitable for color mapping.\n",
    "\n",
    "6. `int_color = np.c_[...]`:\n",
    "   - Purpose: Interpolates the intensity colors using the predefined `VID_RANGE` and `VIRIDIS` colormap.\n",
    "   - Usage: Creates RGB color values corresponding to each point based on its intensity.\n",
    "\n",
    "7. `points = data[:, :-1]`:\n",
    "   - Purpose: Extracts the X, Y, and Z coordinates from the point data.\n",
    "   - Usage: Prepares the coordinates for visualization.\n",
    "\n",
    "8. `points[:, :1] = -points[:, :1]`:\n",
    "   - Purpose: Inverts the X-axis coordinates.\n",
    "   - Usage: Adjusts the coordinate system as needed for visualization.\n",
    "\n",
    "9. `point_list.points = o3d.utility.Vector3dVector(points)`:\n",
    "   - Purpose: Sets the points of the Open3D point cloud object.\n",
    "   - Usage: Prepares the points for rendering in the visualizer.\n",
    "\n",
    "10. `point_list.colors = o3d.utility.Vector3dVector(int_color)`:\n",
    "    - Purpose: Sets the colors of the points in the Open3D point cloud object.\n",
    "    - Usage: Applies the interpolated colors to the corresponding points for visualization.\n",
    "'''\n",
    "def lidar_callback(point_cloud, point_list):\n",
    "    ''' Prepares a point cloud with intensity colors ready to be consumed by Open3D'''\n",
    "    data = np.copy(np.frombuffer(point_cloud.raw_data, dtype = np.dtype('f4')))\n",
    "    data = np.reshape(data, (int(data.shape[0] / 4), 4))\n",
    "\n",
    "    # Isolate the intensity and compute a color for it\n",
    "    intensity = data[:, -1]\n",
    "    intensity_col = 1.0 - np.log(intensity) / np.log(np.exp(-0.004 * 100))\n",
    "    int_color = np.c_[\n",
    "        np.interp(intensity_col, VID_RANGE, VIRIDIS[:, 0]),\n",
    "        np.interp(intensity_col, VID_RANGE, VIRIDIS[:, 1]),\n",
    "        np.interp(intensity_col, VID_RANGE, VIRIDIS[:, 2])]\n",
    "\n",
    "    points = data[:, :-1]\n",
    "    points[:, :1] = -points[:, :1]\n",
    "\n",
    "    point_list.points = o3d.utility.Vector3dVector(points)\n",
    "    point_list.colors = o3d.utility.Vector3dVector(int_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05d49f44-c8e7-45ef-9dc4-156c1535b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Initialize a point cloud object for visualization.\n",
    "\n",
    "1. `point_list = o3d.geometry.PointCloud()`:\n",
    "   - Purpose: Creates a new instance of a PointCloud object in Open3D.\n",
    "   - Usage: This object will hold and manage the 3D point data collected from the LiDAR sensor, enabling visualization and manipulation of the point cloud in the Open3D environment.\n",
    "\n",
    "'''\n",
    "point_list = o3d.geometry.PointCloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f575f7-84bd-4a6b-9c5d-050bd82a649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Objective: Retrieve the dimensions of the camera's image.\n",
    "\n",
    "1. `image_w = camera_bp.get_attribute(\"image_size_x\").as_int()`:\n",
    "   - Purpose: Fetches the width of the camera's image in pixels.\n",
    "   - Usage: Stores the image width, which can be used for processing or displaying the captured images.\n",
    "\n",
    "2. `image_h = camera_bp.get_attribute(\"image_size_y\").as_int()`:\n",
    "   - Purpose: Fetches the height of the camera's image in pixels.\n",
    "   - Usage: Stores the image height, ensuring that the dimensions are available for image processing and visualization tasks.\n",
    "\n",
    "'''\n",
    "image_w = camera_bp.get_attribute(\"image_size_x\").as_int()\n",
    "image_h = camera_bp.get_attribute(\"image_size_y\").as_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e73607-fd92-44ae-93ac-149d20306ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Initialize a dictionary to store camera and depth image data.\n",
    "\n",
    "1. `camera_data = {'image': np.zeros((image_h, image_w, 4)), 'depth_image': np.zeros((image_h, image_w, 4))}`:\n",
    "   - Purpose: Creates a dictionary to hold two types of image data: RGB and depth images.\n",
    "   - Usage: Initializes both images as zero-filled NumPy arrays with dimensions corresponding to the camera's height and width, including an alpha channel. This structure allows for easy storage and access of image data during the simulation\n",
    "'''\n",
    "camera_data = {'image' : np.zeros((image_h, image_w, 4)), 'depth_image': np.zeros((image_h, image_w, 4))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff65b7a-c636-4c61-87c2-ca91ae2f7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Set up a callback function to process incoming camera images.\n",
    "\n",
    "1. `camera.listen(lambda image: camera_callback(image, camera_data))`:\n",
    "   - Purpose: Registers a callback function to be called whenever a new image is received from the camera sensor.\n",
    "   - Parameters:\n",
    "     - `lambda image`: A lambda function that takes the incoming image as input.\n",
    "     - `camera_callback(image, camera_data)`: Calls the `camera_callback` function, passing the incoming image and a dictionary to store the processed image data.\n",
    "   - Usage: Allows for real-time processing and storage of camera images, enabling further analysis or visualization.\n",
    "\n",
    "'''\n",
    "camera.listen(lambda image: camera_callback(image, camera_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5108f964-19cd-4c88-ad56-e2dfad12acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Set up the LiDAR sensor to listen for incoming point cloud data.\n",
    "\n",
    "1. `lidar.listen(lambda data: lidar_callback(data, point_list))`:\n",
    "   - Purpose: Registers a callback function (`lidar_callback`) that will be triggered whenever point cloud data is received from the LiDAR sensor.\n",
    "   - Usage: This lambda function passes the incoming data and the `point_list` object to the `lidar_callback`, facilitating real-time processing and visualization of the LiDAR point cloud as it is generated during the simulation.\n",
    "\n",
    "'''\n",
    "lidar.listen(lambda data: lidar_callback(data, point_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acf60673-d4a3-4476-8aaa-d334fe1edfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Objective: Set up the depth camera to listen for incoming depth images.\n",
    "\n",
    "1. `depth_camera.listen(lambda image: depth_callback(image, camera_data))`:\n",
    "   - Purpose: Registers a callback function (`depth_callback`) that will be triggered whenever a depth image is captured by the depth camera.\n",
    "   - Usage: This lambda function passes the captured depth image and the `camera_data` dictionary to the `depth_callback`, enabling real-time processing of depth data as it becomes available during the simulation.\n",
    "\n",
    "'''\n",
    "\n",
    "depth_camera.listen(lambda image: depth_callback(image, camera_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88b2a109-958c-4431-9237-79065239cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Enable the vehicle's autopilot mode.\n",
    "\n",
    "1. `vehicle.set_autopilot(True)`:\n",
    "   - Purpose: Activates the autopilot feature for the vehicle.\n",
    "   - Usage: Allows the vehicle to autonomously navigate the environment, following traffic rules and pre-defined routes in the Carla simulation, simulating the behavior of self-driving cars.\n",
    "\n",
    "'''\n",
    "vehicle.set_autopilot(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca6bab3-fdea-4002-8636-6ee77f0fdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Objective: Set up visualization windows and render LiDAR data while displaying RGB and depth images in real time.\n",
    "\n",
    "1. `cv2.namedWindow('RGB Camera', cv2.WINDOW_NORMAL)`:\n",
    "   - Purpose: Creates a window named \"RGB Camera\" for displaying RGB images.\n",
    "   - Usage: The `cv2.WINDOW_NORMAL` flag allows the window to be resized.\n",
    "\n",
    "2. `cv2.namedWindow('Depth Camera', cv2.WINDOW_NORMAL)`:\n",
    "   - Purpose: Creates a window named \"Depth Camera\" for displaying depth images.\n",
    "   - Usage: Similar to the RGB window, this window can also be resized.\n",
    "\n",
    "3. `vis = o3d.visualization.Visualizer()`:\n",
    "   - Purpose: Initializes a visualizer object from Open3D for rendering point cloud data.\n",
    "   - Usage: Prepares the visualizer for displaying 3D data.\n",
    "\n",
    "4. `vis.create_window(...)`:\n",
    "   - Purpose: Creates a new window for visualization.\n",
    "   - Parameters:\n",
    "     - `window_name`: Title of the window.\n",
    "     - `width` and `height`: Dimensions of the window.\n",
    "     - `left` and `top`: Position of the window on the screen.\n",
    "   - Usage: Sets up the visual environment for rendering.\n",
    "\n",
    "5. `vis.get_render_option().background_color = [0.05, 0.05, 0.05]`:\n",
    "   - Purpose: Changes the background color of the visualizer window.\n",
    "   - Usage: Provides a dark background to enhance the visibility of the point cloud.\n",
    "\n",
    "6. `vis.get_render_option().point_size = 1`:\n",
    "   - Purpose: Sets the size of the points in the point cloud visualization.\n",
    "   - Usage: Adjusts how the points are rendered for better visibility.\n",
    "\n",
    "7. `vis.get_render_option().show_coordinate_frame = True`:\n",
    "   - Purpose: Displays a coordinate frame in the visualizer.\n",
    "   - Usage: Helps to orient the viewer in 3D space.\n",
    "\n",
    "8. `add_open3d_axis(vis)`:\n",
    "   - Purpose: Calls a function to add a 3D axis to the visualizer.\n",
    "   - Usage: Enhances the context of the visualization with reference axes.\n",
    "\n",
    "9. `frame = 0`:\n",
    "   - Purpose: Initializes a frame counter for controlling the visualization loop.\n",
    "\n",
    "10. `while True:`:\n",
    "    - Purpose: Starts an infinite loop for real-time visualization and image display.\n",
    "\n",
    "11. Inside the loop:\n",
    "    - **Adding Geometry**:\n",
    "      - `if frame == 2: vis.add_geometry(point_list)`: Adds the point cloud geometry to the visualizer on the third iteration.\n",
    "    - **Updating Geometry**:\n",
    "      - `vis.update_geometry(point_list)`: Updates the point cloud data in the visualizer.\n",
    "    - **Polling Events**:\n",
    "      - `vis.poll_events()`: Processes events such as window resizing or closing.\n",
    "    - **Updating Renderer**:\n",
    "      - `vis.update_renderer()`: Renders the current frame.\n",
    "    - **Frame Delay**:\n",
    "      - `time.sleep(0.005)`: Introduces a short delay to control the loop speed.\n",
    "    - **Displaying Images**:\n",
    "      - `cv2.imshow('RGB Camera', camera_data['image'])`: Shows the RGB image in the RGB Camera window.\n",
    "      - `cv2.imshow('Depth Camera', camera_data['depth_image'])`: Shows the depth image in the Depth Camera window.\n",
    "    - **Exit Condition**:\n",
    "      - `if cv2.waitKey(1) == ord('q'):`: Exits the loop when 'q' is pressed.\n",
    "\n",
    "12. **Cleanup**:\n",
    "    - `cv2.destroyAllWindows()`: Closes all OpenCV windows.\n",
    "    - Stops and destroys all sensors and the vehicle:\n",
    "      - `lidar.stop()`, `lidar.destroy()`, `camera.stop()`, `camera.destroy()`, `depth_camera.stop()`, `depth_camera.destroy()`.\n",
    "    - Destroys all vehicle and sensor actors in the simulation world:\n",
    "      - `for actor in world.get_actors().filter('*vehicle*'): actor.destroy()`.\n",
    "      - `for sensor in world.get_actors().filter('*sensor*'): sensor.destroy()`.\n",
    "\n",
    "'''\n",
    "\n",
    "cv2.namedWindow('RGB Camera', cv2.WINDOW_NORMAL)\n",
    "cv2.namedWindow('Depth Camera', cv2.WINDOW_NORMAL)\n",
    "\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(\n",
    "    window_name = 'Carla Lidar',\n",
    "    width = 960,\n",
    "    height = 540,\n",
    "    left = 480,\n",
    "    top = 270)\n",
    "\n",
    "vis.get_render_option().background_color = [0.05, 0.05, 0.05]\n",
    "vis.get_render_option().point_size = 1\n",
    "vis.get_render_option().show_coordinate_frame = True\n",
    "add_open3d_axis(vis)\n",
    "frame = 0\n",
    "\n",
    "while True:\n",
    "    if frame == 2:\n",
    "        vis.add_geometry(point_list)\n",
    "    vis.update_geometry(point_list)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    time.sleep(0.005)\n",
    "    frame +=1\n",
    "    \n",
    "    cv2.imshow('RGB Camera', camera_data['image'])\n",
    "    cv2.imshow('Depth Camera', camera_data['depth_image'])\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "lidar.stop()\n",
    "lidar.destroy()\n",
    "camera.stop()\n",
    "camera.destroy()\n",
    "depth_camera.stop()\n",
    "depth_camera.destroy()\n",
    "for actor in world.get_actors().filter('*vehicle*'):\n",
    "    actor.destroy()\n",
    "for sensor in world.get_actors().filter('*sensor*'):\n",
    "    sensor.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26da00-b190-4d37-bd52-f3d280801358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
